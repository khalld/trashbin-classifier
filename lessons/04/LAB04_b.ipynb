{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LAB04_b.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "46161941e04c4142a07f606b2f32462d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7920b4223488459896136b7afb09005c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_37289b9866e647c2a3b3dfe067d0fd53",
              "IPY_MODEL_77b1f002f1f54b1bb296884e713f39db"
            ]
          }
        },
        "7920b4223488459896136b7afb09005c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "37289b9866e647c2a3b3dfe067d0fd53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_41a7e69f006c4af1bb5bd1036e3107c8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 169001437,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 169001437,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_01fb63d137ff4f148c73364d2dc3507d"
          }
        },
        "77b1f002f1f54b1bb296884e713f39db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5a94d8ac44f94df8a1e8558e3217af91",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 169001984/? [00:13&lt;00:00, 12120682.99it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0d6f914e713640819028abdbe2a65785"
          }
        },
        "41a7e69f006c4af1bb5bd1036e3107c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "01fb63d137ff4f148c73364d2dc3507d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5a94d8ac44f94df8a1e8558e3217af91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0d6f914e713640819028abdbe2a65785": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2QCUUJPDfaH"
      },
      "source": [
        "# Immagini naturali e \"miniAlexNet\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJgTi0A3DDJv"
      },
      "source": [
        "import torch\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "\n",
        "from torch.optim import SGD\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.metrics import accuracy_score\n",
        "from os.path import join"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NznnXtTKGD95",
        "outputId": "6bde1b90-153a-4607-9443-961902e95be5"
      },
      "source": [
        "np.random.seed(1328)\n",
        "torch.random.manual_seed(1328)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fa32d4348f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAq1fVBcG8aE"
      },
      "source": [
        "Importo il train classifier e l'avg meter e il test classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toARofMqI9ff"
      },
      "source": [
        "class AverageValueMeter():\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "    \n",
        "    def reset(self):\n",
        "        self.sum = 0\n",
        "        self.num = 0\n",
        "    \n",
        "    def add(self, value, num):\n",
        "        self.sum += value*num\n",
        "        self.num += num\n",
        "\n",
        "    def value(self):\n",
        "        try:\n",
        "            return self.sum/self.num\n",
        "        except:\n",
        "            return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X03-kTXIJf3N"
      },
      "source": [
        "def train_classifier(model, train_loader, test_loader, exp_name='experiment', lr=0.01, epochs=10, momentum=0.99, logdir='logs'):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = SGD(model.parameters(), lr, momentum=momentum)\n",
        "\n",
        "    # meters\n",
        "    loss_meter = AverageValueMeter()\n",
        "    acc_meter = AverageValueMeter()\n",
        "    # writer\n",
        "    writer = SummaryWriter(join(logdir, exp_name))\n",
        "    # device\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    ## definiamo un dizionario contenente i loader di training e test\n",
        "    loader = {\n",
        "        'train': train_loader,\n",
        "        'test': test_loader\n",
        "    }\n",
        "    global_step = 0\n",
        "    for e in range(epochs):\n",
        "        # iteriamo tra due modalità: train e test\n",
        "        for mode in ['train', 'test']:\n",
        "            loss_meter.reset(); acc_meter.reset()\n",
        "            model.train() if mode == 'train' else model.eval()\n",
        "            with torch.set_grad_enabled(mode=='train'): # abilitiamo i gradienti o solo in training\n",
        "                for i, batch in enumerate(loader[mode]):\n",
        "                    x=batch[0].to(device) # portiamoli su device corretto\n",
        "                    y=batch[1].to(device)\n",
        "                    output = model(x)\n",
        "\n",
        "                    # aggiorniamo il global_step\n",
        "                    # conterrà il numero di campioni visti durante il training\n",
        "                    n = x.shape[0]  # n di elementi nel batch\n",
        "                    global_step += n\n",
        "                    l = criterion(output, y)\n",
        "\n",
        "                    if mode=='train':\n",
        "                        l.backward()\n",
        "                        optimizer.step()\n",
        "                        optimizer.zero_grad()\n",
        "\n",
        "                    acc = accuracy_score(y.to('cpu'), output.to('cpu').max(1)[1])\n",
        "                    loss_meter.add(l.item(), n)\n",
        "                    acc_meter.add(acc,n)\n",
        "\n",
        "                    # loggiamo i risultati iterazione per iterazione solo durante il training\n",
        "                    if mode == 'train':\n",
        "                        writer.add_scalar('loss/train', loss_meter.value(), global_step=global_step)\n",
        "                        writer.add_scalar('accuracy/train', acc_meter.value(), global_step=global_step)\n",
        "\n",
        "                        # una volta finita l'epoca sia nel caso di training che di test loggiamo le stime finali\n",
        "\n",
        "                writer.add_scalar('loss/' + mode, loss_meter.value(), global_step=global_step)\n",
        "                writer.add_scalar('accuracy/' + mode, acc_meter.value(), global_step=global_step)\n",
        "\n",
        "                        ## conserviamo i pesi del modello alla fine di un ciclo di training e test\n",
        "\n",
        "        torch.save(model.state_dict(), '%s-%d.pth'%(exp_name, e+1))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvBzJ5AJJh55"
      },
      "source": [
        "def test_classifier(model, loader):\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  model.to(device)\n",
        "  predictions, labels = [], []\n",
        "  for batch in loader:\n",
        "    x = batch[0].to(device)\n",
        "    y = batch[1].to(device)\n",
        "    output = model(x)\n",
        "    preds = output.to('cpu').max(1)[1].numpy()\n",
        "    labs = y.to('cpu').numpy()\n",
        "    predictions.extend(list(preds))\n",
        "    labels.extend(list(labs))\n",
        "  return np.array(predictions), np.array(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 872
        },
        "id": "lMpeU8ocGDY6",
        "outputId": "22f6961e-f58a-4f54-8b9c-76395060d049"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 103), started 1:54:23 ago. (Use '!kill 103' to kill it.)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6tRNccTD0NE"
      },
      "source": [
        "Carichiamo il dataset CIFRAR-100. Normalizziamo le immagini usando media e varianza per canale che sono state precomputate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "46161941e04c4142a07f606b2f32462d",
            "7920b4223488459896136b7afb09005c",
            "37289b9866e647c2a3b3dfe067d0fd53",
            "77b1f002f1f54b1bb296884e713f39db",
            "41a7e69f006c4af1bb5bd1036e3107c8",
            "01fb63d137ff4f148c73364d2dc3507d",
            "5a94d8ac44f94df8a1e8558e3217af91",
            "0d6f914e713640819028abdbe2a65785"
          ]
        },
        "id": "tU2VRNniDz7-",
        "outputId": "50818425-a799-497f-b61f-0e043d263f93"
      },
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize( ( 0.4914, 0.4822, 0.4465) , (0.2023, 0.1994, 0.2010 ) )])\n",
        "\n",
        "cifar100_train = CIFAR100(root='cifar100',train=True, download=True, transform=transform)\n",
        "cifar100_test = CIFAR100(root='cifar100',train=False, download=True, transform=transform)\n",
        "cifar100_train_loader = DataLoader(cifar100_train, batch_size=1024, num_workers=2, shuffle=True)\n",
        "cifar100_test_loader = DataLoader(cifar100_test, batch_size=1024, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to cifar100/cifar-100-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46161941e04c4142a07f606b2f32462d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=169001437.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting cifar100/cifar-100-python.tar.gz to cifar100\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6-s0oSyEzC4"
      },
      "source": [
        "Ridefiniamo il modello per applicarlo su immagini rgb (LeNetV2) per prendere in input immagini 32x32. Aumenteremo il numero di feature maps e unità nei layer fully connected per aumentare le capacità della rete. Sostituiremo l'average Pooling con il MaxPooling e le attivazioni Tanh e ReLu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT2D9ErYFFmN"
      },
      "source": [
        "from torch import nn\n",
        "class LeNetColor(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LeNetColor, self).__init__()\n",
        "    #ridefiniamo il modello utilizzando i moduli sequential.\n",
        "    #ne definiamo due: un \"feature extractor\", che estrae le feature maps #e un \"classificatore\" che implementa i livelly FC\n",
        "    self.feature_extractor = nn.Sequential(\n",
        "      nn.Conv2d(3, 18, 5), #Input: 3 x 32 x 32. Ouput: 18 x 28 x 28\n",
        "      nn.MaxPool2d(2), #Input: 18 x 28 x 28. Output: 18 x 14 x 14\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(18, 28, 5), #Input 18 x 14 x 14. Output: 28 x 10 x 10\n",
        "      nn.MaxPool2d(2), #Input 28 x 10 x 10. Output: 28 x 5 x 5\n",
        "      nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Linear(700, 360), # Input 28 * 5 * 5\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(360, 252),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(252, 100)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    #applichiamo tutte le diverse trasformazioni in cascata\n",
        "    x = self.feature_extractor(x)\n",
        "    x = self.classifier(x.view(x.shape[0],-1))\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur73xWUYGLKH"
      },
      "source": [
        "Alleniamo di seguito il modello:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTbM8uAdGKm2"
      },
      "source": [
        "lenet_cifar100 = LeNetColor()\n",
        "lenet_cifar100 = train_classifier(lenet_cifar100, cifar100_train_loader, cifar100_test_loader, 'lenet_cifar100', epochs=150)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igvmGNqEQify"
      },
      "source": [
        "Calcoliamo l'accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUlVI8gxQkiw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdda44c6-6a94-4615-8d2b-6938c0628bd3"
      },
      "source": [
        "lenet_cifar100_test_predictions, cifar100_labels_test = test_classifier(lenet_cifar100, cifar100_test_loader)\n",
        "\n",
        "#format to print just 2 decimals instead of all of them when you print a float\n",
        "\n",
        "print(\"Accuracy LeNetColor su CIFAR-100: %0.2f%%\" % \\\n",
        "  accuracy_score(cifar100_labels_test, lenet_cifar100_test_predictions))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy LeNetColor su CIFAR-100: 0.23%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KBwnXk_SRQ7"
      },
      "source": [
        "L'accuracy è piuttosto bassa. Proviamo a migliorarla aumentando la capacità del modello. Definiamo un modello più \"profondo\" con 5 livelli di convoluzione e tre layer fully connected. Inseriremo il max pooling solo tra il primo e secondo livello, il secondo e il terzo, e il quinto e il sesto. Per evitare l'eccessiva riduzione di dimensioni delle mappe di features, specifichiamo un padding pari al ceil della dimensione del kernel fratto 2. Questo farà sì che le convoluzioni non riducano le dimensioni delle mappe di features. Utilizziamo kernel di dimensioni più grandi nei primi layer e più piccoli nei layer successivi. Il modello è vagamente ispirato al modello \"AlexNet\" proposto da Krizhevsky et al. nel 2013."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T75b8a1HSY7S"
      },
      "source": [
        "# MiniAlexNet\n",
        "\n",
        "All'interno delle slide viene introdotto passo passo miniAlexnet. Tuttavia per aspettare delle ore per la fase di training viene implementata di seguito la versione finale di MiniAlexNet.\n",
        "\n",
        "Negli esempi visti nelle slide, quando vi è una grossa differenza tra accuracy di training e di test, è dovuto dall'overfitting e si acuisce quando i modelli contengono molti parametri e i dataset sono piccoli. Oltre alla tecnica di regolarizzazione mediante weight decay, esistono altre tecniche per ridurre l'overfitting quali \n",
        "- dropout\n",
        "- data agumentation\n",
        "- batch normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFj5xjztSQ50"
      },
      "source": [
        "class MiniAlexNetV4(nn.Module):\n",
        "  def __init__(self, input_channels=3, out_classes=100):\n",
        "    super(MiniAlexNetV4, self).__init__()\n",
        "    # ridefiniamo il modello utilizzando i moduli sequential, ne definiamo due: un \"feature extractor\" che estrae le feature maps e un \"classificatore\" implementa i livelli di FC\n",
        "    self.feature_extractor = nn.Sequential(\n",
        "        #Conv1\n",
        "        nn.Conv2d(input_channels, 16, 5, padding=2) # Input: 3x28x28. Output 16x28x28\n",
        "        nn.MaxPool2d(2),\n",
        "        \n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWyK98H0SSwp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}