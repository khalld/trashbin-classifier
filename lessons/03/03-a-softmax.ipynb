{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
   "display_name": "Python 3.9.2 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import Tensor, nn"
   ]
  },
  {
   "source": [
    "# Implementazione di regressore softmax\n",
    "Carichiamo dataset delle iris di fisher. Contenente\n",
    "- 4 quantità relative (features)\n",
    "- 150 fiori (classi)\n",
    "- 3 specie diverse (istanze)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "features (150, 4) \n classi target (150,) \n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X=iris.data\n",
    "Y=iris.target\n",
    "print(\"features\", X.shape,\n",
    "    \"\\n classi target\", Y.shape,\n",
    "    \"\\n\", Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inizializzazione\n",
    "\n",
    "## seed per risultati ripetibili\n",
    "np.random.seed(1234)\n",
    "torch.random.manual_seed(1234)\n",
    "\n",
    "## permutazione casuale dei dati\n",
    "idx = np.random.permutation(len(X))\n",
    "\n",
    "## applico la stessa sia a X che a Y\n",
    "X = X[idx]\n",
    "Y = Y[idx]\n",
    "\n",
    "## suddivido dataset in training e testing set indipendenti e trasformiamo gli array in tensori\n",
    "X_training = Tensor(X[30:])\n",
    "Y_training = Tensor(Y[30:])\n",
    "X_testing = Tensor(X[:30])\n",
    "Y_testing = Tensor(Y[:30])\n",
    "\n",
    "## normalizzo i dati\n",
    "X_mean = X_training.mean(0)\n",
    "X_std = X_training.std(0)\n",
    "\n",
    "X_training_norm = (X_training-X_mean)/X_std\n",
    "X_testing_norm = (X_testing-X_mean)/X_std\n"
   ]
  },
  {
   "source": [
    "Definisco un nuovo modulo per effettuare la regressione softmax "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMaxRegressor(nn.Module):\n",
    "    def __init__(self, in_features, out_classes):\n",
    "        \"\"\"Costruisce un regressore softmax\n",
    "            Input:\n",
    "                in_features: numero di feature in input (es.4)\n",
    "                out_classes: numero di classi in uscita (es.3) \"\"\"\n",
    "        super(SoftMaxRegressor, self).__init__()    ## richiamo costruttore della superclasse, passo necessario per abilitare alcuni meccanismi automatici di PyTorch\n",
    "\n",
    "        self.linear = nn.Linear(in_features, out_classes)   ## il regressore softmax restituisce distr di probabilità, quindi il numero di feature di output coincide con il numero di classi. è lineare in quanto il softmax viene implementato nella loss\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"Definisce come processare l'input x\"\"\"\n",
    "        scores = self.linear(x)\n",
    "        return scores"
   ]
  },
  {
   "source": [
    "Costruiamo un regressore softmax e passiamogli i dati di training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 0.9326,  0.6582, -0.2940],\n",
       "        [-0.1301, -0.2885,  0.1746],\n",
       "        [ 1.1440,  1.6170, -1.0236],\n",
       "        [-0.1766, -0.4061,  0.2196],\n",
       "        [-0.4706, -0.5166,  0.2307],\n",
       "        [ 1.2162,  1.5326, -0.9408],\n",
       "        [ 1.6219,  1.6549, -0.9210],\n",
       "        [ 0.8984,  1.2598, -0.8457],\n",
       "        [ 1.4105,  1.9569, -1.2018],\n",
       "        [ 1.0097,  0.5520, -0.2012]], grad_fn=<SliceBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "## z = torch.Tensor([14,4.3, 100])\n",
    "\n",
    "## implementazione grezza del softmax\n",
    "## def softmax(z):\n",
    "##    z = z-torch.max(z)      ## permette che sia più robusta per i numeri più grandi\n",
    "##    z_exp = torch.exp(z)\n",
    "##    return z_exp/z_exp.sum()\n",
    "\n",
    "## print(softmax(z))\n",
    "\n",
    "model = SoftMaxRegressor(4,3) # 4 feature in ingresso, 3 classi in uscita\n",
    "#mostriamo le prime 4 predizioni\n",
    "model(X_training_norm)[:10]"
   ]
  },
  {
   "source": [
    "ogni riga della matrice è una predizione. Non si tratta di valide distribuzioni di probabilità, per ottenere le distribuzioni usiamo softmax"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "#softmax(model(X_training_norm))[:10]\n",
    "\n",
    "## adesso abbiamo una valida distribuzioni di probabilità sulle tre classi. la somma lunghe le righe è pari a 1 infatti:\n",
    "\n",
    "#softmax(model(X_training_norm)).sum(1)"
   ]
  },
  {
   "source": [
    "una volta allenato, il modello permetterà di predire una distribuzione di probabilità per ogni elemento. per ottenere l'etichetta predetta, applichiamo il principio Maximum A Posteriori (MAP), scegliendo la classe che presenta la probabilità maggiore mediante argmax inclusa in pytorch nella funzione max"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.35833333333333334\n"
     ]
    }
   ],
   "source": [
    "# ritorna i valori dei massimi e i loro indici (il ris della funzione argmax)\n",
    "# per questo includiamo [1] nell'equazione successiva\n",
    "\n",
    "preds = softmax(model(X_training_norm)).max(1)[1]\n",
    "preds\n",
    "\n",
    "## dopo aver ottenuto le predizioni sotto forma di indici delle rte classi che vanno da 0 a 2 possiamo valutare le predizioni come visto nel caso binario. calcoliamo l'accuracy\n",
    "\n",
    "print(accuracy_score(Y_training, preds))"
   ]
  },
  {
   "source": [
    "L'accuracy è molto bassa in quanto dobbiamo ancora allenare il modello\n",
    "\n",
    "Dato che la funzione softmax è monotona, possiamo applicare argmax direttamente ai logits ottenendo lo stesso risultato\n",
    "\n",
    "```\n",
    "preds_logits= model(X_training_norm).max(1)[1]\n",
    "print((preds_logits==preds).float().mean()) #il risultato ottenuto è lo stesso\n",
    "```\n",
    "\n",
    "In pratica si preferisce non applicare softmax per il calcolo delle etichette predette\n",
    "(mancano le formulette)\n",
    "La procedura di training del regressore logistico sarà la seguente:\n",
    "    1.Normalizzare i dati in ingresso  \n",
    "    2. Costruire il modulo che implementa il modello (il costruttore si preoccuperà di inizializzare i parametri)\n",
    "    3. Mettere il modello in modalità \"training\"\n",
    "    4. Calcolare l'output del modello  \n",
    "    5. Calcolare il valore della loss \n",
    "    6. Calcolare il gradiente della loss rispetto ai parametri del modello;\n",
    "    7. Aggiornare i pesi   utilizzando il gradient descent\n",
    "    8. Ripetere i passi 4-7 fino a convergenza.\n",
    "\n",
    "Implementiamo di conseguenza la procedura introducendo il monitoring delle curve tramite tensorboard e calcolo dell'accuracy ad ogni iterazione"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n",
      "Accuracy di training 0.9583333333333334\n",
      "Accuracy di test 1.0\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import SGD\n",
    "\n",
    "writer = SummaryWriter('logs/softmax_regressor')\n",
    "\n",
    "lr = 0.1\n",
    "epochs = 500\n",
    "\n",
    "## normalizzazione dei dati\n",
    "X_mean = X_training.mean(0)\n",
    "X_std = X_training.std(0)\n",
    "\n",
    "X_training_norm = (X_training-X_mean)/X_std\n",
    "X_testing_norm = (X_testing-X_mean)/X_std\n",
    "\n",
    "model = SoftMaxRegressor(4, 3)\n",
    "criterion = nn.CrossEntropyLoss()       # cross-entropy loss\n",
    "optimizer = SGD(model.parameters(), lr)  # optimizer\n",
    "\n",
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    out = model(X_training_norm)\n",
    "    l = criterion(out, Y_training.long())\n",
    "    l.backward()\n",
    "    writer.add_scalar('loss/train', l.item(), global_step=e)\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    preds_train = out.max(1)[1]\n",
    "    writer.add_scalar('accuracy/train', accuracy_score(Y_training, preds_train), global_step=e)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        out = model(X_testing_norm)\n",
    "        l = criterion(out, Y_testing.long())\n",
    "        writer.add_scalar('loss/test', l.item(), global_step=e)\n",
    "        preds_test = out.max(1)[1]\n",
    "        writer.add_scalar('accuracy/test', accuracy_score(Y_testing, preds_test), global_step=e)\n",
    "\n",
    "\n",
    "## Calcolo accuracy di training e test\n",
    "\n",
    "preds_train = model(X_training_norm).max(1)[1]\n",
    "preds_test = model(X_testing_norm).max(1)[1]\n",
    "\n",
    "print(\"Accuracy di training\", accuracy_score(Y_training,preds_train) )\n",
    "print(\"Accuracy di test\", accuracy_score(Y_testing,preds_test) )\n"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}