{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "code-v0.15.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "WYWIiSaM993x",
        "l4cMl3p7PI7L",
        "ruVKhvhePTpP",
        "9vCPd9H2PEkj",
        "G81wrGBg_SPb",
        "w7DV-5iP_DsC",
        "pMqRBDFo_KTD",
        "_DBk0D_y-8iE",
        "l4REKbedDqCE"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYWIiSaM993x"
      },
      "source": [
        "# Libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOFChPb2-UWU"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from os.path import join, splitext\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score  # computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
        "import random\n",
        "\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "from torch.utils import data # necessary to create a map-style dataset https://pytorch.org/docs/stable/data.html\n",
        "from torch import nn    # basic building-blocks for graphs https://pytorch.org/docs/stable/nn.html\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import SGD\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# *** torchvision pretrained models https://pytorch.org/vision/stable/models.html ***\n",
        "from torchvision.models import squeezenet1_0\n",
        "from torchvision.models import alexnet\n",
        "from torchvision.models import vgg16"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NIpTbajCfzR",
        "outputId": "efc018cc-1836-47f1-ea7a-cc18f96caaee"
      },
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uJCjt-bCiLK"
      },
      "source": [
        "random.seed(1996)\n",
        "np.random.seed(1996)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khalzCiImT9e",
        "outputId": "2af6c5ee-406b-41db-e74c-e0fd2f380607"
      },
      "source": [
        "GDRIVE_PATHS = {\n",
        "    'main': '/content/gdrive/MyDrive/trashbean-classifier/',\n",
        "    'dataset': '/content/gdrive/MyDrive/trashbean-classifier/dataset/',\n",
        "    'logs': '/content/gdrive/MyDrive/trashbean-classifier/logs/',\n",
        "    'models': '/content/gdrive/MyDrive/trashbean-classifier/logs/models/'\n",
        "}\n",
        "\n",
        "print(join(GDRIVE_PATHS['logs'], 'test_join'))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/trashbean-classifier/logs/test_join\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vlAWBxemiv_",
        "outputId": "a6cdfb0b-271f-44cc-bfd3-7ca388826bb9"
      },
      "source": [
        "print(type(GDRIVE_PATHS), type(GDRIVE_PATHS['logs']))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'dict'> <class 'str'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7LFiORP-L3m"
      },
      "source": [
        "# Dataset and average value meters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4cMl3p7PI7L"
      },
      "source": [
        "### TrashbeanDst"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzpwIynF-OMh"
      },
      "source": [
        "class TrashbeanDataset(data.Dataset): # data.Dataset https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#Dataset\n",
        "    \"\"\" A map-style dataset class used to manipulate a dataset composed by:\n",
        "        image path of trashbean and associated label that describe the available capacity of the trashbean\n",
        "            0 : empty trashbean\n",
        "            1 : half trashbean\n",
        "            2 : full trashbean\n",
        "\n",
        "        Attributes\n",
        "        ----------\n",
        "        data : str\n",
        "            path of csv file\n",
        "        transform : torchvision.transforms\n",
        "\n",
        "        Methods\n",
        "        -------\n",
        "        __len__()\n",
        "            Return the length of the dataset\n",
        "\n",
        "        __getitem__(i)\n",
        "            Return image, label of i element of dataset  \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, csv=None, transform=None):\n",
        "        \"\"\" Constructor of the dataset\n",
        "            Parameters\n",
        "            ----------\n",
        "            csv : str\n",
        "            path of the dataset\n",
        "\n",
        "            transform : torchvision.transforms\n",
        "            apply transform to the dataset\n",
        "\n",
        "            Raises\n",
        "            ------\n",
        "            NotImplementedError\n",
        "                If no path is passed is not provided a default dataset\n",
        "        \"\"\"\n",
        "        \n",
        "        if csv is None:\n",
        "            raise NotImplementedError(\"No default dataset is provided\")\n",
        "        if splitext(csv)[1] != '.csv':\n",
        "            raise NotImplementedError(\"Only .csv files are supported\")\n",
        "        \n",
        "        self.data = pd.read_csv(csv)        # import from csv using pandas\n",
        "        self.data = self.data.iloc[np.random.permutation(len(self.data))]       # random auto-permutation of the data\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\" Return length of dataset \"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, i=None):\n",
        "        \"\"\" Return the i-th item of dataset\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            i : int\n",
        "            i-th item of dataset\n",
        "\n",
        "            Raises\n",
        "            ------\n",
        "            NotImplementedError\n",
        "            If i is not a int\n",
        "        \"\"\"\n",
        "        if i is None:\n",
        "            raise NotImplementedError(\"Only int type is supported for get the item. None is not allowed\")\n",
        "        \n",
        "        im_path, im_label = self.data.iloc[i]['path'], self.data.iloc[i].label\n",
        "        im = Image.open(im_path)        # Handle image with Image module from Pillow https://pillow.readthedocs.io/en/stable/reference/Image.html\n",
        "        if self.transform is not None:\n",
        "            im = self.transform(im)\n",
        "        return im, im_label"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruVKhvhePTpP"
      },
      "source": [
        "### TsbContainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0BtukhsO7rN"
      },
      "source": [
        "class TDContainer:\n",
        "    \"\"\" Class that contains the dataset for training, validation and test\n",
        "        Attributes\n",
        "        ----------\n",
        "        self.training, self.validation, self.test are the TrashbeanDataset object\n",
        "        self.training_loader, self.validation_loader, self.test_loader are DataLoader of the correspective TrashbeanDataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, training=None, validation=None, test=None):\n",
        "        \"\"\" Constructor of the class. Instantiate an Trashbean dataset for each dataset\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            training: str, required\n",
        "                path of training dataset csv\n",
        "            \n",
        "            validation: str, required\n",
        "                path of validation dataset csv\n",
        "            \n",
        "            test: str, required\n",
        "                path of test dataset csv\n",
        "        \"\"\"\n",
        "    \n",
        "        if training is None or validation is None or test is None:\n",
        "            raise NotImplementedError(\"No default dataset is provided\")\n",
        "        \n",
        "        if isinstance(training, dict) is False or isinstance(validation, dict) is False or isinstance(test, dict) is False:\n",
        "            raise NotImplementedError(\"Constructor accept only dict file.\")\n",
        "\n",
        "        if training['path'] is None or validation['path'] is None or test['path'] is None or isinstance(training['path'], str) is False or isinstance(validation['path'], str) is False or isinstance(test['path'], str) is False:\n",
        "            raise NotImplementedError(\"Path file is required and need to be a str type.\")\n",
        "\n",
        "        self.training = TrashbeanDataset(training['path'], transform=training['transform'])\n",
        "        self.validation = TrashbeanDataset(validation['path'], transform=validation['transform'])\n",
        "        self.test = TrashbeanDataset(test['path'], transform=test['transform'])\n",
        "        self.hasDl = False\n",
        "\n",
        "    def create_data_loader(self, _batch_size=32, _num_workers=2, _drop_last=False):\n",
        "        \"\"\" Create data loader for each dataset\n",
        "\n",
        "            https://pytorch.org/docs/stable/data.html\n",
        "            \n",
        "            Parameters\n",
        "            ----------\n",
        "\n",
        "            _batch_size: int\n",
        "                number of batches, default 32\n",
        "\n",
        "            _num_workers: int\n",
        "                number of workers\n",
        "        \"\"\"\n",
        "\n",
        "        if isinstance(_batch_size, int) is False or isinstance(_num_workers, int) is False:\n",
        "            raise NotImplementedError(\"Parameters accept only int value.\")\n",
        "\n",
        "        self.training_loader = DataLoader(self.training, batch_size=_batch_size, num_workers=_num_workers, drop_last=_drop_last, shuffle=True)\n",
        "        self.validation_loader = DataLoader(self.validation, batch_size=_batch_size, num_workers=_num_workers, drop_last=_drop_last)\n",
        "        self.test_loader = DataLoader(self.test, batch_size=_batch_size, num_workers=_num_workers, drop_last=_drop_last)\n",
        "        self.hasDl = True\n",
        "\n",
        "    def show_info(self):\n",
        "        \"\"\" Print info of dataset \"\"\"\n",
        "        print(\"\\n=== *** DB INFO *** ===\")\n",
        "        print(\"Training:\", self.training.__len__(), \"values, \\nValidation:\", self.validation.__len__(), \"values, \\nTest:\", self.test.__len__())\n",
        "        print(\"DataLoader:\", self.hasDl)\n",
        "        print(\"=== *** END *** ====\\n\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vCPd9H2PEkj"
      },
      "source": [
        "### Avg value meter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7gNiytIO-P7"
      },
      "source": [
        "class AverageValueMeter():\n",
        "    \"\"\"Calculate Average Value Meter\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.sum = 0\n",
        "        self.num = 0\n",
        "\n",
        "    def add(self, value, num):\n",
        "        self.sum += value*num\n",
        "        self.num += num\n",
        "\n",
        "    def value(self):\n",
        "        try:\n",
        "            return self.sum/self.num\n",
        "        except:\n",
        "            return None"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWNEi823-ACo"
      },
      "source": [
        "# Pretrained class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G81wrGBg_SPb"
      },
      "source": [
        "## Creator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q2DpoD499k3"
      },
      "source": [
        "from __future__ import annotations\n",
        "from abc import ABC, abstractmethod # https://docs.python.org/3/library/abc.html\n",
        "\n",
        "# Creator\n",
        "class PretrainedModelsCreator(ABC):\n",
        "    \"\"\"The Creator class declares the factory method that is supposed to return an\n",
        "    object of a Product class. The Creator's subclasses usually provide the\n",
        "    implementation of this method.\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def factory_method(self):\n",
        "        \"\"\" No default implementation needed\"\"\"\n",
        "        pass\n",
        "\n",
        "    def initialize_dst(self, dataset, output_class: int = 2, dl_attributes: dict = {'batch_size': 32, 'num_workers': 2, 'drop_last': False}) -> None:\n",
        "        \"\"\"\n",
        "        The Creator's primary responsibility is not creating products. Usually, it contains\n",
        "        some core business logic that relies on Product objects, returned by the factory method.\n",
        "        Subclasses can indirectly change that business logic by overriding the\n",
        "        factory method and returning a different type of product from it.\n",
        "        \"\"\"\n",
        "\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        # call factory method to create a Product object\n",
        "        product = self.factory_method()\n",
        "        # get the model from product\n",
        "        self.model = product.get_model(output_class)\n",
        "        # set the dataset inside the object\n",
        "        self.dst = dataset\n",
        "        ## instantiate DataLoader too\n",
        "        self.dst.create_data_loader(_batch_size=dl_attributes['batch_size'], _num_workers=dl_attributes['num_workers'], _drop_last=dl_attributes['drop_last'] )        \n",
        "\n",
        "    def trainval_classifier(self, exp_name='experiment', lr=0.01, epochs=10, momentum=0.99,\n",
        "                            log_dir='logs',\n",
        "                            models_dir='models',\n",
        "                            train_from_epoch=0, save_on_runtime=False, save_each_iter=20):\n",
        "        \n",
        "        model = self.model\n",
        "        timer_start = time.time()    \n",
        "        \n",
        "        criterion = nn.CrossEntropyLoss() # used for classification https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "        \n",
        "        optimizer = SGD(model.parameters(), lr, momentum=momentum)\n",
        "\n",
        "        # meters\n",
        "        loss_meter = AverageValueMeter()\n",
        "        acc_meter = AverageValueMeter()\n",
        "\n",
        "        # writer\n",
        "        writer = SummaryWriter(join(log_dir, exp_name))\n",
        "\n",
        "        model.to(self.device)\n",
        "        ## definiamo un dizionario contenente i loader di training e test\n",
        "        loader = {\n",
        "            'train': self.dst.training_loader,\n",
        "            'validation': self.dst.validation_loader\n",
        "        }\n",
        "        global_step = 0\n",
        "        print(\"Computing epoch:\")\n",
        "        for e in range(epochs):\n",
        "            print(e+1, \"/\", epochs, \"... \")\n",
        "            # iteriamo tra due modalità: train e test\n",
        "            for mode in ['train', 'validation']:\n",
        "                loss_meter.reset(); acc_meter.reset()\n",
        "                model.train() if mode == 'train' else model.eval()\n",
        "                with torch.set_grad_enabled(mode=='train'): # abilitiamo i gradienti o solo in training\n",
        "                    for i, batch in enumerate(loader[mode]):\n",
        "                        x = batch[0].to(self.device) # portiamoli su device corretto\n",
        "                        y = batch[1].to(self.device)\n",
        "                        output = model(x)\n",
        "\n",
        "                        # aggiorniamo il global_step\n",
        "                        # conterrà il numero di campioni visti durante il training\n",
        "                        n = x.shape[0]  # n di elementi nel batch\n",
        "                        global_step += n\n",
        "                        l = criterion(output, y)\n",
        "\n",
        "                        if mode == 'train':\n",
        "                            l.backward()\n",
        "                            optimizer.step()\n",
        "                            optimizer.zero_grad()\n",
        "\n",
        "                        acc = accuracy_score(y.to('cpu'), output.to('cpu').max(1)[1])\n",
        "                        loss_meter.add(l.item(), n)\n",
        "                        acc_meter.add(acc,n)\n",
        "\n",
        "                        # loggiamo i risultati iterazione per iterazione solo durante il training\n",
        "                        if mode == 'train':\n",
        "                            writer.add_scalar('loss/train', loss_meter.value(), global_step=global_step)\n",
        "                            writer.add_scalar('accuracy/train', acc_meter.value(), global_step=global_step)\n",
        "\n",
        "                    # una volta finita l'epoca sia nel caso di training che di test loggiamo le stime finali\n",
        "                    writer.add_scalar('loss/' + mode, loss_meter.value(), global_step=global_step)\n",
        "                    writer.add_scalar('accuracy/' + mode, acc_meter.value(), global_step=global_step)\n",
        "\n",
        "            # conserviamo i pesi del modello alla fine di un ciclo di training e test..\n",
        "            # ...sul runtime\n",
        "            if save_on_runtime is True:\n",
        "                torch.save(model.state_dict(), '%s-%d.pth'%(exp_name, (e+1) + train_from_epoch ) )\n",
        "\n",
        "            # ...ogni save_each_iter salvo il modello sul drive per evitare problemi di spazio su Gdrive\n",
        "            if ((e+1) % save_each_iter == 0 or (e+1) % 50 == 0):\n",
        "                torch.save(model.state_dict(), models_dir + '%s-%d.pth'%(exp_name, (e+1) + train_from_epoch ) )\n",
        "\n",
        "        timer_end = time.time()\n",
        "        print(\"Ended in: \", ((timer_end - timer_start) / 60 ), \"minutes\" )\n",
        "        return model\n",
        "\n",
        "\n",
        "    def test_classifier(self, model, dataLoader):  # self.dataLoader\n",
        "        model.to(self.device)\n",
        "        predictions, labels = [], []\n",
        "        for batch in dataLoader:\n",
        "            x = batch[0].to(self.device)\n",
        "            y = batch[1].to(self.device)\n",
        "            output = model(x)\n",
        "            preds = output.to('cpu').max(1)[1].numpy()\n",
        "            labs = y.to('cpu').numpy()\n",
        "            predictions.extend(list(preds))\n",
        "            labels.extend(list(labs))\n",
        "        return np.array(predictions), np.array(labels)\n",
        "\n",
        "\n",
        "    def train(self, parameters, paths, train_from_epoch, save_on_runtime, save_each_iter) -> None:\n",
        "\n",
        "        self.model_finetuned = self.trainval_classifier(exp_name=parameters['exp_name'], lr=parameters['lr'], epochs=parameters['epochs'],\n",
        "                                                        momentum=parameters['momentum'],\n",
        "                                                        log_dir=paths['logs'],\n",
        "                                                        models_dir=paths['models'],\n",
        "                                                        train_from_epoch=train_from_epoch,\n",
        "                                                        save_on_runtime=save_on_runtime,\n",
        "                                                        save_each_iter=save_each_iter\n",
        "                                                        )\n",
        "\n",
        "        print(\"**** Training procedure ended. Start to calculate accuracy ...\")\n",
        "\n",
        "        self.model_finetuned_predictions_test, self.dataset_labels_test = self.test_classifier(self.model_finetuned, self.dst.test_loader)\n",
        "        print(\"Accuracy of \" + parameters['exp_name'] + \" %0.2f%%\" % (accuracy_score(self.dataset_labels_test, self.model_finetuned_predictions_test)*100) )\n",
        "\n",
        "\n",
        "    def load_model(self, path: str) -> None:\n",
        "        print(\"Loading model using load_state_dict..\")\n",
        "        self.model.load_state_dict(torch.load(path))\n",
        "\n",
        "    def get_info(self) -> None:\n",
        "        print(\"Information about model:\\n\", self.model)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7DV-5iP_DsC"
      },
      "source": [
        "## Concrete creators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvlXqCHf_EAt"
      },
      "source": [
        "\"\"\" Concrete Creators override the factory method in order to change the resulting product's type. \"\"\"\n",
        "\n",
        "# ConcreteCreator1 \n",
        "class CCSqueezeNet(PretrainedModelsCreator):\n",
        "    def factory_method(self) -> PretrainedModel:\n",
        "        return CPSqueezeNet()\n",
        "\n",
        "# ConcreteCreator2\n",
        "class CCAlexNet(PretrainedModelsCreator):\n",
        "    def factory_method(self) -> PretrainedModel:\n",
        "        return CPAlexNet()\n",
        "\n",
        "# ConcreteCreator3\n",
        "class CCVgg16(PretrainedModelsCreator):\n",
        "    def factory_method(self) -> PretrainedModel:\n",
        "        return CPVgg16()\n"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMqRBDFo_KTD"
      },
      "source": [
        "## Product"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8fJevR0_LPK"
      },
      "source": [
        "# Product\n",
        "class PretrainedModel(ABC):\n",
        "    \"\"\" The Product interface declares the operations that all concrete products\n",
        "    must implement.\"\"\"\n",
        "    @abstractmethod\n",
        "    def get_model(self, output_class: int = 3):\n",
        "        pass"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DBk0D_y-8iE"
      },
      "source": [
        "## Concrete products"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY32AzPi-_9K"
      },
      "source": [
        "\"\"\"Concrete Products provide various implementations of the Product interface.\"\"\"\n",
        "\n",
        "# ConcreteProduct1\n",
        "class CPSqueezeNet(PretrainedModel):\n",
        "    def get_model(self, output_class: int = 3):\n",
        "        model = squeezenet1_0(pretrained=True)\n",
        "        model.classifier[1] = nn.Conv2d(512, output_class, kernel_size=(1,1), stride=(1,1))\n",
        "\n",
        "        return model\n",
        "    \n",
        "# ConcreteProduct2\n",
        "class CPAlexNet(PretrainedModel):\n",
        "    def get_model(self, output_class: int = 3):\n",
        "        model = alexnet(pretrained=True)\n",
        "        model.classifier[6] = nn.Linear(4096, output_class) # https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
        "\n",
        "        return model\n",
        "\n",
        "# ConcreteProduct3\n",
        "class CPVgg16(PretrainedModel):\n",
        "    def get_model(self, output_class: int = 3):\n",
        "        model = vgg16(pretrained=True)\n",
        "        model.classifier[6] = nn.Linear(4096, output_class)\n",
        "\n",
        "        return model"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE7PzgB2-FZr"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4REKbedDqCE"
      },
      "source": [
        "## Load del dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZUOQci8-oJP"
      },
      "source": [
        "train_transform = transforms.Compose([\n",
        "  transforms.Resize(256),\n",
        "  transforms.RandomCrop(224),\n",
        "  transforms.RandomHorizontalFlip(),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "  transforms.Resize(256),\n",
        "  transforms.CenterCrop(224), # crop centrale\n",
        "  transforms.RandomHorizontalFlip(),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "trashbean_dataset = TDContainer({\n",
        "  \"path\": join(GDRIVE_PATHS['dataset'], 'training.csv'),\n",
        "  \"transform\": train_transform\n",
        "},{\n",
        "  \"path\": join(GDRIVE_PATHS['dataset'], 'validation.csv'),\n",
        "  \"transform\": test_transform\n",
        "}, {\n",
        "    \"path\": join(GDRIVE_PATHS['dataset'], 'test.csv'),\n",
        "    \"transform\": test_transform\n",
        "})"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnXfrTYYDe4O",
        "outputId": "b7bb0926-ecfc-4160-cfd4-946d9f4ed592"
      },
      "source": [
        "trashbean_dataset.show_info()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "=== *** DB INFO *** ===\n",
            "Training: 900 values, \n",
            "Validation: 900 values, \n",
            "Test: 900\n",
            "DataLoader: False\n",
            "=== *** END *** ====\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoJ_xvOiD2sZ"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qz2I0BjU-G-0"
      },
      "source": [
        "def do_training(creator: PretrainedModelsCreator, dataset: TDContainer, output_class: int, dl_attributes: dict, parameters: dict, paths: dict, train_from_epoch: int=0, save_on_runtime: bool=True, save_each_iter:int=20 ) -> None:\n",
        "    creator.initialize_dst(dataset, output_class, dl_attributes)\n",
        "    creator.train(parameters=parameters, paths=paths, train_from_epoch=train_from_epoch, save_on_runtime=save_on_runtime, save_each_iter=save_each_iter)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahgXhqe39Nv7"
      },
      "source": [
        "def do_loading(loaded_model, creator: PretrainedModelsCreator, dataset: TDContainer, output_class: int, dl_attributes: dict, parameters: dict, paths: dict, train_from_epoch: int=0, save_on_runtime: bool=True, save_each_iter:int=20 ) -> None:\n",
        "  creator.initialize_dst(dataset, output_class, dl_attributes)\n",
        "  creator.load_model(loaded_model)\n",
        "  creator.train(parameters=parameters, paths=paths, train_from_epoch=train_from_epoch, save_on_runtime=save_on_runtime, save_each_iter=save_each_iter)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9L6k8D1D4wN"
      },
      "source": [
        "# trashbean_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k--IfjjCzHNv"
      },
      "source": [
        "GDRIVE_PATHS = {\n",
        "    'main': '/content/gdrive/MyDrive/trashbean-classifier/',\n",
        "    'dataset': '/content/gdrive/MyDrive/trashbean-classifier/dataset/',\n",
        "    'logs': '/content/gdrive/MyDrive/trashbean-classifier/logs/',\n",
        "    'models': '/content/gdrive/MyDrive/trashbean-classifier/logs/models/'\n",
        "}"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP6R6K6hEbWk"
      },
      "source": [
        "# using the Concrete Creator\n",
        "print(\"App: Launching training with the SqueezeNet.\")\n",
        "do_training(creator=CCSqueezeNet(), dataset=trashbean_dataset, output_class=3, dl_attributes={'batch_size': 32, 'num_workers': 2, 'drop_last': False}, parameters={'exp_name': 'SqueezeNet__v1', 'lr': 0.001, 'epochs': 1, 'momentum': 0.99 }, paths=GDRIVE_PATHS, train_from_epoch=0, save_on_runtime=True, save_each_iter=1)\n",
        "\n",
        "### print(\"App: Launching training with Alexnet.\")\n",
        "### do_training(CCAlexNet(), trashbean_dataset)\n",
        "### \n",
        "### print(\"App: Launching trainin with VGG16.\")\n",
        "### do_training(CCVgg16(), trashbean_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ypm3ryG-9qyC",
        "outputId": "be18f183-65b9-44e8-8acd-bbe1c2fb8bed"
      },
      "source": [
        "print(\"App: Reloading training with the SqueezeNet.\")\n",
        "do_loading('/content/gdrive/MyDrive/trashbean-classifier/logs/models/SqueezeNet__v1-1.pth', creator=CCSqueezeNet(), dataset=trashbean_dataset, output_class=3, dl_attributes={'batch_size': 32, 'num_workers': 2, 'drop_last': False}, parameters={'exp_name': 'SqueezeNet__v1', 'lr': 0.001, 'epochs': 1, 'momentum': 0.99 }, paths=GDRIVE_PATHS, train_from_epoch=1, save_on_runtime=True, save_each_iter=1)\n"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "App: Reloading training with the SqueezeNet.\n",
            "Loading model using load_state_dict..\n",
            "Computing epoch:\n",
            "1 / 1 ... \n",
            "Ended in:  1.0224265416463216 minutes\n",
            "**** Training procedure ended. Start to calculate accuracy ...\n",
            "Accuracy of SqueezeNet__v1 55.89%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc5eqFRMQ8yf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}