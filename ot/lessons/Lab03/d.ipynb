{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
   "display_name": "Python 3.9.2 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from PIL import Image\n",
    "from os import path\n",
    "from torchvision import transforms\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "# Costruzione di un oggetto dataset personalizzato\n",
    "\n",
    "Consideriamo il dataset a [questo link] (http://people.csail.mit.edu/torralba/code/spatialenvelope/spatial_envelope_256x256_static_8outdoorcategories.zip)\n",
    "\n",
    "2688 immagini a colori 256x256 suddivise in 8 classi a secondo del tipo di scena del ritratto\n",
    "- forest\n",
    "- highway\n",
    "- insidecity\n",
    "- mountain\n",
    "- opencountry\n",
    "- street\n",
    "- tallbuilding\n",
    "\n",
    "All'interno della cartella sono presenti 3 file testo\n",
    "- `train.txt`: 2188 immagini di training con relative etichette in formato numerico (0-7)\n",
    "- `test.txt`: contiene i nomi delle rimanenti 500 immagini di testing con le relative etichette in formato numerico\n",
    "- `classes.txt`: nomi delle 3 classi. La i-esima conterrà il nome della classe i-esima\n",
    "\n",
    "Costruiamo di conseguenza un oggetto Dataset che ci permetta di caricare le immagini di training e test. Ciò si può fare in maniera naturale in PyTorch ereditando dalla classe Dataset. Ogni oggetto deve contenere almeno\n",
    "- costruttore\n",
    "- metodo __len__ che restituisce il numero di elementi contenuti nel dataset\n",
    "- metodo __getitem__ che prende in input un indice i e restituisce l'i-esimo elemento del dataset\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScenesDataset(Dataset):\n",
    "    \"\"\"Implementa l'oggetto scenesDataset che ci permette di caricare le immagini del dataset 8 scenes\"\"\"\n",
    "    def __init__(self, base_path, txt_list, transform=None):\n",
    "        \"\"\"Input:\n",
    "        base_path: path della cartella contenente le immagini\n",
    "        txt_list: il path al file del testo contenente la lista delle immagini con le relative etichette. Ad esempio train.txt o test.txt\n",
    "        transform: implementeremo il dataset in modo che esso supporti le trasformazioni \"\"\"\n",
    "        # conserviamo il path alla cartella contenente le immagini\n",
    "        self.base_path = base_path\n",
    "        # carichiamo la lista dei file\n",
    "        # sarà una matrice con n righe (n di immagini) e 2 colonne (path, etichetta)\n",
    "\n",
    "        self.images = np.loadtxt(txt_list, dtype=str, delimiter=',')\n",
    "        # conserviamo il riferimento alla trasformazione da applicare\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # recuperiamo il path dell'immagine di indice index e la relativa etichetta\n",
    "        f,c = self.images[index]\n",
    "\n",
    "        # carichiamo l'immagine utilizzando PIL\n",
    "        im = Image.open(path.join(self.base_path, f))\n",
    "\n",
    "        # se la trasformazione è definita, applichiamola all'immagine\n",
    "        if self.transform is not None:\n",
    "            im = self.transform(im)\n",
    "        \n",
    "        # converto l'etichetta in intero\n",
    "        label = int(c)\n",
    "\n",
    "        #restituisco un dizionario contenente immagine etichetta\n",
    "        return {'image': im, 'label': label}\n",
    "\n",
    "    # restituisce numero di campioni: la linghezza della lista images\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "source": [
    "Istanzio il dataset per caricare dei dati"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 256, 256])\n1\n"
     ]
    }
   ],
   "source": [
    "dataset = ScenesDataset('8scenes','8scenes/train.txt',transform=transforms.ToTensor())\n",
    "sample = dataset[0]\n",
    "#l'immagine è 3 x 256 x 256 perché è una immagine a colori\n",
    "print(sample['image'].shape)\n",
    "print(sample['label'])"
   ]
  },
  {
   "source": [
    "Le immagini sono di dimensioni 256x256. Per ridurre i tempi computazionali, potremmo voler lavorare con immagini più piccole usando trasformazione Resize"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 32, 32]) 1\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.Resize(32), transforms.ToTensor()])\n",
    "dataset = ScenesDataset('8scenes','8scenes/train.txt', transform=transform)\n",
    "sample = dataset[0]\n",
    "print(sample['image'].shape, sample['label'])"
   ]
  },
  {
   "source": [
    "Per poter normalizzare i dati calcoliamo medie e varianza di tutti i pixel contenuti nelle immagini del dataset. Nel caso di immagini a colori vanno calcolati canale per canale:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Medie: [0.42478886 0.45170452 0.4486708 ] \n dev std: [0.25579566 0.24652381 0.27658252]\n"
     ]
    }
   ],
   "source": [
    "dataset = ScenesDataset('8scenes', '8scenes/train.txt', transform=transforms.ToTensor())\n",
    "m = np.zeros(3)\n",
    "for sample in dataset:\n",
    "    m+= sample['image'].sum(1).sum(1).numpy() ## accumulo somma dei pixel canale per canale\n",
    "\n",
    "# divido per il numero di immagini moltiplicato per il n di pixel\n",
    "m = m/(len(dataset)*256*256)\n",
    "\n",
    "# procedura simile per calcolare dev std\n",
    "s = np.zeros(3)\n",
    "for sample in dataset:\n",
    "    s+=((sample['image']-torch.Tensor(m).view(3,1,1))**2).sum(1).sum(1).numpy()\n",
    "\n",
    "s=np.sqrt(s/(len(dataset)*256*256))\n",
    "\n",
    "\n",
    "print(\"Medie:\", m, \"\\n dev std:\", s)"
   ]
  },
  {
   "source": [
    "Inseriamo la corretta normalizzazione tra le trasformazioni"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3072]) 1\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.Resize(32), transforms.ToTensor(), transforms.Normalize(m,s), transforms.Lambda(lambda x: x.view(-1))])\n",
    "\n",
    "dataset = ScenesDataset('8scenes', '8scenes/train.txt', transform=transform)\n",
    "\n",
    "print(dataset[0]['image'].shape, dataset[0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}