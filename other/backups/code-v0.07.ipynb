{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "code-new.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kSjKJbPvHQm_",
        "b7DHe6NNYH6T"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7jGNPbmG8i9"
      },
      "source": [
        "#### Domande\n",
        "- il numero di immagini è corretto?\n",
        "- Dobbiamo fare object detection? E quindi classificare il dataset con i bounding box o non è necessario?\n",
        "- Alla fine di tutto come dovremmo utilizzare il modello? Su una web-app ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNZ7ujrsGBSW"
      },
      "source": [
        "- Cambiare il dataset come visto in lezione 5\n",
        "- Partiamo dal presupposto che abbiamo fatto training da modelli inizializzati randomicamente prima\n",
        "- Uno fa alexNet e uno squeezenet (pretrained)\n",
        "- Vediamo la differenza tra alexNet pretrained e no (vecchio codice) anche se cmq i layer del modello sono diversi.\n",
        "- DA PULIRE CODICE .PY che estrae i frame\n",
        "- Dobbiamo sistemare la nostra dev std o quella del modello?\n",
        "- leggi domanda su paragrafo squeezenet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSjKJbPvHQm_"
      },
      "source": [
        "# Preparazione del dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjIwwzWnJXz1"
      },
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils import data"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RR7PSAMRHo9u",
        "outputId": "dcd4275e-3d84-4caf-9eb1-53c806ef15ac"
      },
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh8CsvjrJHjN"
      },
      "source": [
        "Tutte le labels sono presenti in un file txt chiamato all_labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsb2AlDZIck8"
      },
      "source": [
        "from glob import glob\n",
        "from os.path import basename\n",
        "\n",
        "all_labels_txt = '/content/gdrive/MyDrive/dataset_progetto/labels/all_labels.txt'\n",
        "\n",
        "path_images = '/content/gdrive/MyDrive/dataset_progetto/img/'\n",
        "\n",
        "# elenco delle classi\n",
        "\n",
        "dataset_from_txt = np.loadtxt(all_labels_txt, dtype=str, delimiter=',')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfoxUWzHK90x"
      },
      "source": [
        "all_images = []\n",
        "all_labels = []\n",
        "\n",
        "for i in range(len(dataset_from_txt)):\n",
        "  all_images.append(path_images + dataset_from_txt[i][0])\n",
        "  all_labels.append(int(dataset_from_txt[i][1]))"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "RcziX1gXKpWp",
        "outputId": "4cce301e-bb0f-4503-cb6b-9e1c10c5ac37"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset = pd.DataFrame({'path': all_images, 'label': all_labels})\n",
        "dataset.head()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/gdrive/MyDrive/dataset_progetto/img/t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/gdrive/MyDrive/dataset_progetto/img/t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/gdrive/MyDrive/dataset_progetto/img/t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/gdrive/MyDrive/dataset_progetto/img/t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/gdrive/MyDrive/dataset_progetto/img/t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                path  label\n",
              "0  /content/gdrive/MyDrive/dataset_progetto/img/t...      0\n",
              "1  /content/gdrive/MyDrive/dataset_progetto/img/t...      0\n",
              "2  /content/gdrive/MyDrive/dataset_progetto/img/t...      0\n",
              "3  /content/gdrive/MyDrive/dataset_progetto/img/t...      0\n",
              "4  /content/gdrive/MyDrive/dataset_progetto/img/t...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsTI41NmQ8x4"
      },
      "source": [
        "Implemento un dizionario che mappi le classi su id numerici:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsVnYvspQ6Wi"
      },
      "source": [
        "dictionary_dataset = {\n",
        "    0: \"empty\",\n",
        "    1: \"half\",\n",
        "    2: \"full\"\n",
        "}\n",
        "\n",
        "# verifico di aver caricato tutto correttamente\n",
        "#Image.open(dictionary_dataset['path'][234])\n",
        "#dictionary[dictionary_dataset['label'][234]]"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXmLpAfcRphm"
      },
      "source": [
        "Il dataframe adesso contiene tutte le informazioni per caricare i dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqOfH27tRjd9"
      },
      "source": [
        "def split_train_val_test(dataset, perc=[0.6, 0.1, 0.3]):\n",
        "  assert sum(perc)==1\n",
        "  train, testval = train_test_split(dataset, test_size = perc[1]+perc[2])\n",
        "  val, test = train_test_split(testval, test_size = perc[2]/(perc[1]+perc[2]))\n",
        "  return train, val, test"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLjJtnJgRtDO",
        "outputId": "8d99d4f7-9da6-4ea4-8bb9-b3481d4b715d"
      },
      "source": [
        "random.seed(12345)\n",
        "np.random.seed(12345)\n",
        "train, val, test = split_train_val_test(dataset)\n",
        "\n",
        "print(\"training:\", len(train), \"\\nvalidation\", len(val), \"\\ntest\", len(test) )"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training: 540 \n",
            "validation 90 \n",
            "test 270\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2SjmWtGSjkz"
      },
      "source": [
        "Salvo i tre dataFrame su csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq0wsjC4SfSS"
      },
      "source": [
        "train.to_csv('/content/gdrive/MyDrive/dataset_progetto/train.csv', index=None)\n",
        "val.to_csv('/content/gdrive/MyDrive/dataset_progetto/valid.csv', index=None)\n",
        "test.to_csv('/content/gdrive/MyDrive/dataset_progetto/test.csv', index=None)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dcFVNyxT-c4"
      },
      "source": [
        "salvo una corrispondenza tra le classi in un altro file csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPRja4GiT7TN"
      },
      "source": [
        "ids, classes = zip(*dictionary_dataset.items())\n",
        "ids = pd.DataFrame({'id':ids, 'class':classes}).set_index('id')\n",
        "ids.to_csv('/content/gdrive/MyDrive/dataset_progetto/classes.csv')"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "RkGV9lNzXHB5",
        "outputId": "12456da0-7339-4ee4-90bc-ea1dc783f16f"
      },
      "source": [
        "ids"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>empty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>half</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>full</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    class\n",
              "id       \n",
              "0   empty\n",
              "1    half\n",
              "2    full"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqPMb8WuUdpB"
      },
      "source": [
        "Creo una classe per caricare il csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck_r0OigUoY2"
      },
      "source": [
        "class TrashbeanDataset(data.Dataset):\n",
        "  def __init__(self, csv, transform=None):\n",
        "    self.data = pd.read_csv(csv)\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  \n",
        "  def __getitem__(self, i):\n",
        "    im_path, im_label = self.data.iloc[i]['path'], self.data.iloc[i].label\n",
        "    \n",
        "    im = Image.open(im_path)\n",
        "\n",
        "    if self.transform is not None:\n",
        "      im = self.transform(im)\n",
        "\n",
        "    return im, im_label"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5E6sBEAV8xy"
      },
      "source": [
        "Carico le classi da file e trasformo il dataframe in un dizionario"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFX82CmyV6SV"
      },
      "source": [
        "classes = pd.read_csv('/content/gdrive/MyDrive/dataset_progetto/classes.csv').to_dict()['class']"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60Ox21Y3VnAb"
      },
      "source": [
        "Carico il dataset dal csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2g3kC9XVmPj"
      },
      "source": [
        "dataset_train = TrashbeanDataset('/content/gdrive/MyDrive/dataset_progetto/train.csv')\n",
        "dataset_valid = TrashbeanDataset('/content/gdrive/MyDrive/dataset_progetto/valid.csv')\n",
        "dataset_test = TrashbeanDataset('/content/gdrive/MyDrive/dataset_progetto/test.csv')"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37aFpR5RXiPb"
      },
      "source": [
        "Testo che sia visualizzato correttamente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAZh6H_iWNQI"
      },
      "source": [
        "#im, lab = dataset_train[0]\n",
        "#print('Class id:',lab, 'Class name:',classes[lab])\n",
        "#im"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7DHe6NNYH6T"
      },
      "source": [
        "# Codice per allenare il modello"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkzj-_uLYJrQ"
      },
      "source": [
        "from torch.optim import SGD\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.metrics import accuracy_score\n",
        "from os.path import join\n",
        "\n",
        "class AverageValueMeter():\n",
        "  def __init__(self):\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self):\n",
        "    self.sum = 0\n",
        "    self.num = 0\n",
        "  \n",
        "  def add(self, value, num):\n",
        "    self.sum += value*num\n",
        "    self.num += num\n",
        "\n",
        "  def value(self):\n",
        "    try:\n",
        "      return self.sum/self.num\n",
        "    except:\n",
        "      return None"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-CDEYGrYLoW"
      },
      "source": [
        "def trainval_classifier(model, train_loader, validation_loader, exp_name='experiment', lr=0.01, epochs=10, momentum=0.99, logdir='logs'):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = SGD(model.parameters(), lr, momentum=momentum)\n",
        "\n",
        "    # meters\n",
        "    loss_meter = AverageValueMeter()\n",
        "    acc_meter = AverageValueMeter()\n",
        "    # writer\n",
        "    writer = SummaryWriter(join(logdir, exp_name))\n",
        "    # device\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    ## definiamo un dizionario contenente i loader di training e test\n",
        "    loader = {\n",
        "        'train': train_loader,\n",
        "        'validation': validation_loader\n",
        "    }\n",
        "    global_step = 0\n",
        "    for e in range(epochs):\n",
        "        # iteriamo tra due modalità: train e test\n",
        "        for mode in ['train', 'validation']:\n",
        "            loss_meter.reset(); acc_meter.reset()\n",
        "            model.train() if mode == 'train' else model.eval()\n",
        "            with torch.set_grad_enabled(mode=='train'): # abilitiamo i gradienti o solo in training\n",
        "                for i, batch in enumerate(loader[mode]):\n",
        "                    x=batch[0].to(device) # portiamoli su device corretto\n",
        "                    y=batch[1].to(device)\n",
        "                    output = model(x)\n",
        "\n",
        "                    # aggiorniamo il global_step\n",
        "                    # conterrà il numero di campioni visti durante il training\n",
        "                    n = x.shape[0]  # n di elementi nel batch\n",
        "                    global_step += n\n",
        "                    l = criterion(output, y)\n",
        "\n",
        "                    if mode=='train':\n",
        "                        l.backward()\n",
        "                        optimizer.step()\n",
        "                        optimizer.zero_grad()\n",
        "\n",
        "                    acc = accuracy_score(y.to('cpu'), output.to('cpu').max(1)[1])\n",
        "                    loss_meter.add(l.item(), n)\n",
        "                    acc_meter.add(acc,n)\n",
        "\n",
        "                    # loggiamo i risultati iterazione per iterazione solo durante il training\n",
        "                    if mode == 'train':\n",
        "                        writer.add_scalar('loss/train', loss_meter.value(), global_step=global_step)\n",
        "                        writer.add_scalar('accuracy/train', acc_meter.value(), global_step=global_step)\n",
        "\n",
        "                        # una volta finita l'epoca sia nel caso di training che di test loggiamo le stime finali\n",
        "\n",
        "                writer.add_scalar('loss/' + mode, loss_meter.value(), global_step=global_step)\n",
        "                writer.add_scalar('accuracy/' + mode, acc_meter.value(), global_step=global_step)\n",
        "\n",
        "                        ## conserviamo i pesi del modello alla fine di un ciclo di training e test\n",
        "\n",
        "        torch.save(model.state_dict(), '%s-%d.pth'%(exp_name, e+1))\n",
        "    return model"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrI9ItcMYNfk"
      },
      "source": [
        "def test_classifier(model, loader):\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  model.to(device)\n",
        "  predictions, labels = [], []\n",
        "  for batch in loader:\n",
        "    x = batch[0].to(device)\n",
        "    y = batch[1].to(device)\n",
        "    output = model(x)\n",
        "    preds = output.to('cpu').max(1)[1].numpy()\n",
        "    labs = y.to('cpu').numpy()\n",
        "    predictions.extend(list(preds))\n",
        "    labels.extend(list(labs))\n",
        "  return np.array(predictions), np.array(labels)"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jh3nd5dZtRu"
      },
      "source": [
        "# Squeezenet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eez1KaZoaNL3"
      },
      "source": [
        "Adattiamo il modello"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmOuGfgHaOVV"
      },
      "source": [
        "from torch import nn\n",
        "from torchvision.models import squeezenet1_0\n",
        "model = squeezenet1_0(pretrained=True)\n",
        "num_class = 3\n",
        "model.classifier[1] = nn.Conv2d(512, num_class, kernel_size=(1, 1), stride=(1, 1))\n",
        "model.num_classes = num_class"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJR4LX4LbWqb"
      },
      "source": [
        "**il primo test sarà fatto su immagini 256x256 (ma le nostre immagini sono 1920x1080). In questo caso conviene fare il resize prima di allenare il modello o converebbe adattare una cnn???** **Inoltre le medie che stiamo applicando sono quelle del codice visto a lezione. In questo caso (noi già abbiamo calcolato le medie nel vecchio colab (RCORDA DI IMPORARE QUELLA PARTE DI CODICE), converebbe utilizzarle?)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNOX1GPkcB-K"
      },
      "source": [
        "from torchvision import transforms\n",
        "train_transform = transforms.Compose([\n",
        "  transforms.Resize(256),\n",
        "  transforms.RandomCrop(224),\n",
        "  transforms.RandomHorizontalFlip(),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZfqfx7TcD8r"
      },
      "source": [
        "test_transform = transforms.Compose([\n",
        "  transforms.Resize(256),\n",
        "  transforms.CenterCrop(224),\n",
        "  transforms.RandomHorizontalFlip(),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjljrCjWcOAi"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset_train = TrashbeanDataset('/content/gdrive/MyDrive/dataset_progetto/train.csv', transform=train_transform)\n",
        "dataset_valid = TrashbeanDataset('/content/gdrive/MyDrive/dataset_progetto/valid.csv', transform=train_transform)\n",
        "dataset_test = TrashbeanDataset('/content/gdrive/MyDrive/dataset_progetto/test.csv', transform=train_transform)\n",
        "\n",
        "dataset_train_loader = DataLoader(dataset_train, batch_size=32, num_workers=2, shuffle=True)\n",
        "dataset_valid_loader = DataLoader(dataset_valid, batch_size=32, num_workers=2)\n",
        "dataset_test_loader = DataLoader(dataset_test, batch_size=32, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWAdV_30cps5"
      },
      "source": [
        "Lanciamo il training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoa-YeJ4cpQg"
      },
      "source": [
        "squeezenet_v1 = get_model()\n",
        "\n",
        "squeezenet_v1_finetuned = trainval_classifier(squeezenet_v1, dataset_train_loader, dataset_valid_loader, \\\n",
        "                                                      exp_name='squeezenet_v1_finetuning', lr=0.001, epochs=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3Mllbe7cuRZ"
      },
      "source": [
        "Calcoliamo l'accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etkSjXlgcyq-"
      },
      "source": [
        "#predizioni di test del modello all'ultima epoch\n",
        "squeezenet_v1_finetuned_predictions_test, dataset_labels_test = \\ \n",
        "  test_classifier(squeezenet_v1_finetuned, dataset_test_loader)\n",
        "\n",
        "print(\"Accuracy di Squeezenet_v1: %0.2f%%\" % \\ \n",
        "       (accuracy_score(dataset_labels_test, squeezenet_v1_finetuned_predictions_test)*100,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UvRuWCaciE5"
      },
      "source": [
        "# Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuIf1dLicjDU"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}